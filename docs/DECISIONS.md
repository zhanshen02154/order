# 微服务架构演进实践——订单服务决策记录

## 集成DTM分布式事务组件
### 日期
2025年11月18日
### 状态
已采纳
### 背景
系统需要使用分布式事务保证数据一致性，项目框架版本为2.9.1。

### 方案A
部署DTM到基础设施服务器，深度修改Go micro 2.9.1框架并集成低版本DTM，根据go-zero的原理移植驱动到go micro来集成。

#### 优点
- 任何层的代码都无需改动，减少集成带来的风险。

#### 缺点
- 虽然可以适当修改框架底层源码但风险较高，框架版本较旧具有许多不确定性。

### 方案B
- DTM部署在K8S集群，利用K8S自带的服务发现访问GRPC服务以解决服务调用，框架保持不变。

#### 优点
- 部署在K8S集群有效利用2台node子节点以缓解基础设施服务器资源紧张。
- 内置服务发现同样能调用GRPC服务。

#### 缺点
- 主要问题仍集中在框架，由于DTM组件较新，引入会修改或移除Go micro 2.9.1涉及的依赖，改造困难。

### 方案C
- 切换到go-zero或kratos确保无缝集成。

#### 优点
- go-zero和kratos有现成的组件可供使用，可以和DTM无缝衔接，虽然需要切换框架但更换后集成难度大幅下降。

#### 缺点
- 由于go micro是高度可插拔的框架，切换为go-zero或kratos耗时长
- 所有代码必须全部重写，代价较大。

### 最终方案
结合方案A和方案B的折中方案：
- 将DTM部署到K8S集群。
- 放弃go micro自带的GRPC客户端，用K8S内置的服务发现机制让DTM调用GRPC服务。
- 升级Go micro框架到v4.11.0版本。

### 采纳理由
- 利用K8S内置服务发现机制可保证DTM能访问到GRPC服务。
- DTM部署到K8S集群减缓基础设施服务器压力，让node节点的资源得以有效利用。
- 升级go micro到4.11.0版本防止DTM依赖影响框架底层，保证客户端顺利引入。
- 由于服务采用DDD领域驱动架构，修改范围仅限于main函数和基础设施层，其余无需改动，改动幅度小于A、B、C。
- 虽然切换到go-zero和kratos也可以但该服务2周迭代一版频率较高暂时不考虑。

### 关于方案C未被采纳的原因和解释
- 目前可观测性仍存在缺陷，4.0版开始将完善基础设施，目前先保证可观测性和基本可用性。
- 3.0版将转向事件驱动架构，go-zero对于事件驱动的支持更加友好开发效率更高但该项目旨在熟悉kafka及事件驱动架构设计故暂时选用轻量级框架。
#### 迁移至go-zero时间点
- 在引入Jaeger、Prometheus、ELK之后。
- 压测后确定go micro v4存在严重性能瓶颈且严重影响到事件驱动。
- Go micro v4存在重大安全隐患。
- 优化难度显著高于切换至go-zero的代价。

## 分布式锁
### 日期
2025年11月13日
### 状态
已采纳
### 背景
订单支付回调API接口压测时存在并发请求问题需要阻止并发操作。
### 方案A
用Redis的SETNX实现分布式锁，将缓存限制在128M。
#### 优点
- Redis分布式锁讨论较多，又有现成的组件redlock，以往的项目经验大都使用Redis实现。
- Redis在内存操作性能高。
#### 缺点
- 当前基础设施服务器配置2核4GB内存，运行Consul、ETCD、MySQL等多个组件，再部署Redis性能将急剧下降。
### 方案B
用ETCD实现分布式锁，ETCD的租约、修订机制和K/V存储可以实现分布式锁。
#### 优点
- ETCD高可用，强一致性的特点结合锁自动续期即可实现，无需部署Redis。
- ETCD客户端v3版本支持分布式锁，已有golang组件且支持阻塞和非阻塞两种锁，可满足基本业务需求。
#### 缺点
- 服务器配置低可能对ETCD的稳定性造成负面影响。
### 最终方案
用ETCD实现分布式锁
### 采纳理由
当前服务器资源紧张无法再部署Redis，
### 风险及应对措施
- ETCD进行高频I/O操作内存上升极快，后续根据迭代的压测结果考虑迁移到K8S集群并调整优化参数，其余服务使用HPA。
- 将ETCD迁移到K8S后将服务注册/发现由Consul替换为ETCD进一步降低资源开销。

## 解决Consul.Watch超时问题
### 日期
2025年11月6日
### 状态
已采纳
### 背景
订单服务出现大量consul健康检查错误的日志，过程主要出在consul.watch，而且请求Consul的API后面带有index参数，经判断为阻塞查询。
### 解决方案
设置Consul全局的超时时间为85s，API接口客户端WaitTime为60s，确保全局超时时间大于WaitTime。
#### 采纳理由
WaitTime等待时间小于全局超时时间就不会报错，不设置则默认5分钟远大于原定的超时时间30s，故配置为85s。

## 优化健康检查
### 日期
2025年11月4日
### 状态
已采纳
### 背景
旧版服务健康检查探针不够完善，只是单独开启Goroutine启动健康检查探针服务，要改成平滑退出。
### 方案A
用context.WithCancel在收到关闭信号后关闭goroutine。
#### 优点
- 使用context精确控制探针服务。
#### 风险
- 由于健康检查探针服务需要使用channel接收系统信号会和go micro框架底层冲突。
### 方案B
使用协程配合sync.WaitGroup控制服务启停，对服务注入BeforeStop函数，在停止服务之前关闭健康检查服务。
#### 优点
- 无需控制复杂的context，直接采用框架自带的方法处理，避免和框架的生命周期管理冲突。
#### 风险
- 无
### 最终方案：方案B
#### 采纳理由
- 不会和底层冲突，实现复杂度低。

---

## 调整目录结构
### 日期
2025年10月31日
### 状态
已采纳
### 背景
原项目方案仅有领域层且排列较为杂乱耦合较高，需要让每层可插拔符合领域驱动架构分层的目标。
### 解决方案
保留领域层，增加应用层、配置、基础设施层，将基础设施分为缓存、客户端、持久化、注册中心共4个包，区分应用层的service和领域层的service，前者用于编排，后者实现具体业务逻辑。
#### 采纳理由
- 将每层做成可插拔的组件提高扩展性，符合领域驱动架构的要求。
- 精简无用的代码，移除私有仓库依赖。
#### 风险及应对措施
- 首版必须对所有功能做完整地测试。
- 涉及频繁转换结构体但可使用对象池避免频繁创建和销毁临时对象。
